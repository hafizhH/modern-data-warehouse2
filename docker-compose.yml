services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    ports:
      - "9864:9864"
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env

  spark-master:
    image: custom-spark:latest
    container_name: spark-master
    restart: always
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/src:/opt/spark/src
      - spark_logs:/opt/bitnami/spark/logs

  spark-worker:
    image: custom-spark:latest
    container_name: spark-worker
    restart: always
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_MEMORY=4G        # Increase memory to 8GB for shared resources
      - SPARK_WORKER_CORES=2         # Allocate 4 CPU cores
    volumes:
      - ./spark/src:/opt/spark/src
      - spark_logs:/opt/bitnami/spark/logs

  # spark-app-processor:
  #   image: custom-spark:latest
  #   container_name: spark-app-processor
  #   build:
  #     context: spark
  #     dockerfile: Dockerfile
  #   restart: on-failure
  #   depends_on:
  #     - spark-master
  #     - namenode
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #   volumes:
  #     - ./spark/src:/opt/spark/src
  #     - ./spark/logs:/opt/bitnami/spark/logs    # Local logs directory
  #     - spark_logs:/opt/bitnami/spark/logs # Shared volume for logs
  #   command: >
  #     /bin/bash -c "
  #     mkdir -p /opt/bitnami/spark/logs &&
  #     pip install requests &&
  #     /opt/bitnami/spark/bin/spark-submit 
  #     --master spark://spark-master:7077
  #     --deploy-mode client
  #     --name ExchangeRateETL
  #     --conf spark.hadoop.fs.defaultFS=hdfs://namenode:9000
  #     /opt/spark/src/exchange_rate_processor.py 2>&1 | tee /opt/bitnami/spark/logs/app.log"

  spark-app-processor:
    image: custom-spark:latest
    container_name: spark-app-processor
    build:
      context: spark
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - spark-master
      - namenode
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./spark/src:/opt/spark/src
      - ./spark/logs:/opt/bitnami/spark/logs
      - spark_logs:/opt/bitnami/spark/logs
    command: >
      /bin/bash -c "
      mkdir -p /opt/bitnami/spark/logs &&
      pip install requests &&
      /opt/bitnami/spark/bin/spark-submit 
      --master spark://spark-master:7077
      --deploy-mode client
      --name ExchangeRateETL
      --conf spark.hadoop.fs.defaultFS=hdfs://namenode:9000
      --conf spark.cores.max=1
      --driver-memory 1G
      --executor-memory 2G
      --executor-cores 1
      --driver-cores 1
      /opt/spark/src/exchange_rate_processor.py 2>&1 | tee /opt/bitnami/spark/logs/exchange_rate_processor.log"

  spark-app-forecast:
    image: custom-spark:latest
    container_name: spark-app-forecast
    build:
      context: spark
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - spark-master
      - namenode
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./spark/src:/opt/spark/src
      - ./spark/logs:/opt/bitnami/spark/logs
      - spark_logs:/opt/bitnami/spark/logs
    command: >
      /bin/bash -c "
      mkdir -p /opt/bitnami/spark/logs &&
      pip install requests &&
      /opt/bitnami/spark/bin/spark-submit 
      --master spark://spark-master:7077
      --deploy-mode client
      --name ExchangeRateForecast
      --conf spark.hadoop.fs.defaultFS=hdfs://namenode:9000
      --conf spark.cores.max=1
      --driver-memory 1G
      --executor-memory 2G
      --executor-cores 1
      --driver-cores 1
      /opt/spark/src/exchange_rate_forecasting.py 2>&1 | tee /opt/bitnami/spark/logs/exchange_rate_forecasting.log"

volumes:
  hadoop_namenode:
  hadoop_datanode:
  spark_logs: